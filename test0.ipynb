{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segyio\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from sgykit import Sgykit\n",
    "from config import Config\n",
    "from generatadata import save_dataT, save_data\n",
    "from dataset import SingleSample\n",
    "from checkdata import CheckData\n",
    "from resunet3d import Res18Unet3D\n",
    "from unet3d import Unet3D\n",
    "from evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgy shape is : (210, 381, 1001)\n",
      "Inline can be sliced 1 section. There are 82 remainder.\n",
      "Crossline can be sliced 2 section. There are 125 remainder.\n",
      "Time can be sliced 7 section. There are 105 remainder.\n",
      "Blocks is 14\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "checker = CheckData()\n",
    "evlter = Evaluate()\n",
    "unet3d = Unet3D(cfg).to(cfg.device)\n",
    "unet3d.load_state_dict(torch.load('checkpoint/unet/0best.mdl'))\n",
    "# unet3d = Res18Unet3D(cfg).to(cfg.device)\n",
    "# unet3d.load_state_dict(torch.load('checkpoint/resunet/best.mdl'))\n",
    "fileName = 'D:\\Work\\Project\\Geomodeling\\CB\\CB_depth\\sample.segy'\n",
    "# fileName = 'D:\\Work\\Project\\Geomodeling\\CB\\CB_depth\\Chengbei_2020_3D_psdm_final_depth.segy'\n",
    "# fileName = 'D:\\Work\\Project\\Geomodeling\\CB\\CB_depth\\Chengbei_C3.sgy'\n",
    "sgykit = Sgykit(fileName,\n",
    "                sliceSize={'iline': 128, 'xline': 128, 'time': 128},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80010, 1001)\n",
      "(x,y) locations: 80010 80010\n",
      "troce numbers: (80010,)\n",
      "sample_interval: 4.0 ms\n",
      "C1  BEGIN EBCDIC LINE HEADER                                                    \n",
      "C2                                                                              \n",
      "C3  Data generated by: AASPI, The University of Oklahoma, Norman, OK, USA       \n",
      "C4  File generated on 04/13/2016 at time 11:12                                  \n",
      "C5                                                                              \n",
      "C6  value of 1st samp in s    samp incr in 1.E-6*s            no. of samples    \n",
      "C7                     0.000                    4000                    1001    \n",
      "C8  binary input AASPI format file name =                                       \n",
      "C9  /ouhomes2/qi1400/image_processing/d_mig_westcam.H                           \n",
      "C10     first line no.     last line no.   line index incr  line incr in m      \n",
      "C11                181               390                 1            25.011    \n",
      "C12      first cdp no.      last cdp no.    cdp index incr   cdp incr in m      \n",
      "C13                190               570                 1            25.011    \n",
      "C14                                                                             \n",
      "C15     inline azimuth crossline azimuth                                        \n",
      "C16            180.000            90.000                                        \n",
      "C17                                                                             \n",
      "C18 Trace header locations:                                                     \n",
      "C19 header variable       byte      type                                        \n",
      "C20 cdp x coordinate   :  181     I32                                           \n",
      "C21 cdp y coordinate   :  185     I32                                           \n",
      "C22 inline number      :  189     I32                                           \n",
      "C23 xline (cdp) number :  193     I32                                           \n",
      "C24                                                                             \n",
      "C25                                                                             \n",
      "C26                                                                             \n",
      "C27 coord scale factor in bytes 71-72 copied from input data                    \n",
      "C28                                                                             \n",
      "C29                                                                             \n",
      "C30                                                                             \n",
      "C31                                                                             \n",
      "C32                                                                             \n",
      "C33                                                                             \n",
      "C34                                                                             \n",
      "C35                                                                             \n",
      "C36                                                                             \n",
      "C37                                                                             \n",
      "C38                                                                             \n",
      "C39                                                                             \n",
      "C40 END EBCDIC LINE HEADER                                                      \n"
     ]
    }
   ],
   "source": [
    "sgykit.sgy_detail(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> float32 (210, 381, 1001)\n"
     ]
    }
   ],
   "source": [
    "data = sgykit.read3D(fileName)\n",
    "print(type(data), data.dtype, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 256, 896)\n"
     ]
    }
   ],
   "source": [
    "path = 'data/sample/seis/'\n",
    "gendb = sgykit.split_sgy(data, path,\n",
    "                        #  False,\n",
    "                         True,\n",
    "                         size={'i': 128, 'x': 128, 't': 128},\n",
    "                         bias={'i': 40, 'x': 0, 't': 0})\n",
    "save_data(gendb, 'data/sample/seis/', 'gendb.datt')\n",
    "print(gendb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redb = sgykit.reconstruct_sgy('data/sample/seis/')\n",
    "checker.check_slice_data(gendb, redb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization successful! This folder hive 14.dat\n"
     ]
    }
   ],
   "source": [
    "sgset = SingleSample('seis', 'data/sample/seis/', (128, 128, 128), 1, '.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet3d.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(sgset.fileNums):\n",
    "        sgdata = sgset.getitem(i).to(cfg.device)\n",
    "        pred = unet3d(sgdata)\n",
    "        # print(type(pred), pred.shape)\n",
    "        prednp = evlter._flatten2np(pred)\n",
    "        save_data(prednp, 'data/sample/pred/resunet/', f'{i}.dat')\n",
    "        save_data(prednp, 'data/sample/pred/resunet/', f'{i}.dat')\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "redb = sgykit.reconstruct_sgy('data/sample/pred/resunet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(redb, 'data/sample/pred/resunet/', 'predb.datt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 去边缘效应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def get_gaussian(s=(128, 128, 128), sigma=1.0/8) -> np.ndarray:\n",
    "\ttemp = np.zeros(s)\n",
    "\t# print('temp:', temp)\n",
    "\t# print('sigma:', sigma)\n",
    "\tcoords = [i // 2 for i in s]\n",
    "\t# print('coords:', coords)\n",
    "\tsigmas = [i * sigma for i in s]\n",
    "\t# print('sigmas:', sigmas)\n",
    "\ttemp[tuple(coords)] = 1\n",
    "\t# print('tuple(coords):', tuple(coords))\n",
    "\t# print('temp[tuple(coords)]:', temp[(1, 1)])\n",
    "\tgaussian_map = gaussian_filter(temp, sigmas, 0, mode='constant', cval=0)\n",
    "\tgaussian_map /= np.max(gaussian_map)\n",
    "\treturn gaussian_map\n",
    "\n",
    "# get_gaussian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_sgy(\n",
    "        data:np.ndarray,\n",
    "        # fileName,\n",
    "        # dataSize,\n",
    "        patchSize=(128, 128, 128)):\n",
    "    \n",
    "    patchSize = np.array(patchSize)\n",
    "    patchStride = patchSize // 2\n",
    "    # data = np.fromfile(fileName, dtype=np.single)\n",
    "    # data = data.reshape(dataSize)\n",
    "\n",
    "    result = np.zeros(data.shape)\n",
    "    normalization = np.zeros(data.shape)\n",
    "    gaussian_map = get_gaussian(patchSize)\n",
    "    for i in range(0, data.shape[0]-patchSize[0]+1, patchStride[0]):\n",
    "        for j in range(0, data.shape[1]-patchSize[1]+1, patchStride[1]):\n",
    "            for k in range(0, data.shape[2]-patchSize[2]+1, patchStride[2]):\n",
    "                patch = data[i:i+patchSize[0],\n",
    "                             j:j+patchSize[1],\n",
    "                             k:k+patchSize[2]].astype(np.float32)\n",
    "                patch *= gaussian_map\n",
    "                normalization[i:i+patchSize[0],\n",
    "                              j:j+patchSize[1],\n",
    "                              k:k+patchSize[2]] += gaussian_map\n",
    "                result[i:i+patchSize[0],j:j+patchSize[1],k:k+patchSize[2]] += patch\n",
    "    result /= normalization\n",
    "    return result.astype(np.single)\n",
    "# plt.imshow(result,cmap=plt.get_cmap('gray'))\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29360128,)\n",
      "(128, 256, 896)\n",
      "0.99972767\n",
      "(128, 256, 896)\n",
      "0.99972767\n"
     ]
    }
   ],
   "source": [
    "path = 'data/sample/pred/predb.datt'\n",
    "data = np.fromfile(path, dtype=np.single)\n",
    "print(data.shape)\n",
    "data = data.reshape(128, 256, 896)\n",
    "print(data.shape)\n",
    "print(data.max())\n",
    "data1 = smooth_sgy(data)\n",
    "print(data1.shape)\n",
    "print(data1.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29360128,)\n"
     ]
    }
   ],
   "source": [
    "data1.tofile('data/sample/pred/predb_s.datt')\n",
    "data2 = np.fromfile('data/sample/pred/predb_s.datt', dtype=np.single)\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "path = 'data/sample/pred/resunet/predb.datt'\n",
    "data = np.fromfile(path, dtype=np.single)\n",
    "data = torch.tensor(data)\n",
    "data1 = torch.where(data>0.7, data, 0)\n",
    "data1 = data1.numpy()\n",
    "data1.tofile('data/sample/pred/resunet/predb70.datt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29360128,)\n"
     ]
    }
   ],
   "source": [
    "path = 'data/sample/pred/predb80.datt'\n",
    "data = np.fromfile(path, dtype=np.single)\n",
    "data1 = data.reshape(128, 256, 896)\n",
    "print(data2.shape)\n",
    "data2 = smooth_sgy(data1)\n",
    "data2.tofile('data/sample/pred/predb80_s.datt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 取单个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, hashlib, csv, glob\n",
    "from evaluate import Evaluate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from config import Config\n",
    "from unet3d import Unet3D, BalancedCrossEntropyLoss, BatchBalancedCrossEntropyLoss\n",
    "from resunet3d import Res18Unet3D\n",
    "from dataset import FaultData, SingleSample\n",
    "from checkdata import CheckData\n",
    "evlat = Evaluate()\n",
    "cfg = Config()\n",
    "checker = CheckData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet3d = Unet3D(cfg).to(cfg.device)\n",
    "unet3d = Res18Unet3D(cfg).to(cfg.device)\n",
    "unet3d.load_state_dict(torch.load('checkpoint/resunet/best.mdl'))\n",
    "optimizer = optim.Adam(unet3d.parameters(), lr=cfg.lr)\n",
    "# criteon = nn.BCELoss()\n",
    "# criteon = nn.CrossEntropyLoss().to(cfg.device)\n",
    "# criteon = BalancedCrossEntropyLoss().to(cfg.device)\n",
    "criteon = BatchBalancedCrossEntropyLoss().to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization successful! This folder hive 800.dat\n",
      "torch.Size([1, 1, 128, 128, 128]) torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "path = './data/train/0seis/'\n",
    "id = 100\n",
    "ss = SingleSample('seis', path).getitem(id).to(cfg.device)\n",
    "print(ss.shape, ss.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "unet3d.eval()\n",
    "with torch.no_grad():\n",
    "    pred = unet3d(ss)\n",
    "pred = pred.squeeze().cpu().numpy().transpose()\n",
    "print(pred.shape)\n",
    "pred.tofile('data/demo/ori128.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 重建训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgykit import Sgykit\n",
    "sgykit = Sgykit()\n",
    "redb = sgykit.reconstruct_sgy('data/validation/fault/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640, 128)\n"
     ]
    }
   ],
   "source": [
    "print(redb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generatadata import save_data\n",
    "save_data(redb, 'data/validation/fault/', 'redb.datt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch002",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
